I"0<p>This is a short summary of <a href="http://papers.nips.cc/paper/5782-character-levelconvolutional-networks-for-text-classifica">Character-level Convolutional Networks for Text
Classification by Zhang et al., 2015</a>.</p>

<p>The paper’s motivation is to see whether character-level convolutional networks (ConvNets) classify texts in a manner
that is competitive with previous, traditional models.</p>

<p>In order to test this hypothesis, texts are broken down to character-level and treated as raw signals, before 1-
Dimensional ConvNets are applied. Temporal Max-Pooling allows for the networks to be trained deeper than 6 layers.
The ConvNets take a one-hot encoding as input and this sequence of encoded characters is then transformed into a
vector. Two ConvNets were designed, one large and one small, both 9 layers deep, with 6 convolutional layers and 3
connected layers. I didn’t understand why the character quantization order was backwards; given that the layers are
already fully connected, shouldn’t the weights be associated with the proper readings regardless of this order? An
alphabet of 70 characters is used as the input features and the dataset is augmented by replacing words with entries
from a thesaurus. At this point, I did not understand why the input feature length was stated as 1014 but listed as 1024
in Table 1. To test and compare the performance of the ConvNets, different learning models were also used and trained
on the same data. These include Bag-Of-Words and its TFIDF, Bag-Of-N-Grams and its TFIDF, Bag-Of-Means on word
embedding, Word-based ConvNets and Long Short-Term Memory (an example of a recurrent neural network model).
Since ConvNets work well on large-scale data, different large-scale datasets were built to test their performances. These
were made by pulling samples from several different sources like Yelp and Amazon. Table 4 displays the testing errors on
all the datasets for all the different models used.</p>

<p>The paper also experimented with using different alphabets, both distinguishing and not distinguishing between uppercase and lower-case letters. The former led to worse results in the ConvNets.
The paper presents two tables to discuss: relative errors with comparison models, and first layer weights. Using these as
evidence, it concludes that a character-level ConvNet is an effective model, comparable to existing models, for text
classification without needing words or knowledge of the syntactic or semantic structures of a language. However, the
measure of its effectiveness is not definitive, as the model’s performance may depend on different factors like dataset
size, curation of the text, and alphabet choice. Typically, larger datasets performed better with ConvNets, and other
models worked better with smaller datasets. The ConvNets were also good for less curated, user generated data. The
semantics of the task may also not matter, and bag-of-means performs worse in all cases. Ultimately, the paper
concludes that no single machine learning model can work for all kinds of data.</p>

<p>This paper appears to successfully present and argue the validity of character-level ConvNets, testing them against a
range of different models and on large-scale datasets. However, given that ConvNets don’t necessarily perform better
on all types of datasets, it might be interesting to further justify how character-level ConvNets add to the current
landscape of text classification. Another path to explore might be the different language datasets, like the Chinese Sogou
news corpus.</p>

<p>The paper suggests ignoring OOV items by treating them as white spaces. This doesn’t appear to be a very effective
tactic for word modelling as the weights can have important implications. The fact that no one method does well across
the datasets is not very surprising as they are all dependant on a variety of different factors, and different models do
better in different settings and tasks. This could suggest that there is a different factor intrinsic to every model that
determines its performance. The table appears justified as it presents data found. However, it could be better
presented, as the lack of error values make it difficult to compare the behaviours of all the models. Perhaps, instead,
there could be two tables, with two different measures for the models, or variance could be included.</p>
:ET